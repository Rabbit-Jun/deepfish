{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최적화\n",
    "- 손실함수의 값을 작게 만드는 파라미터를 찾는 것\n",
    "\n",
    "# 모멘텀\n",
    "- v = av -lr(v에 대한 L의 미분)\n",
    "- w = w +v\n",
    "\n",
    "# 확률적 감소 (SGD)\n",
    "- w -= lr(w에 대한 L의 미분)\n",
    "\n",
    "\n",
    "# adaGrad\n",
    "- h += (w에 대한 L의 미분)+(w에 대한 L의 미분)\n",
    "- w -= lr*np.sqrt(h) * (w에 대한 L의 미분)\n",
    "\n",
    "# 지수이동평균\n",
    "- adaGrad에서 갈수록 가중치의 갱신값이 줄어드는 것을 막기위해 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영\n",
    "\n",
    "# adam\n",
    "- 모멘텀과 adaGrad를 합친 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 가중치 감소\n",
    "- 매개변수의 값이 작아지도록 학습하여 오버피팅이 일어나지 않게\n",
    "- 단, 가중치의 초기값을 0으로 하면 역전파를 해도 그 다음 역전파에서 다른 노드들과 같은 값을 가지게 됨 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
